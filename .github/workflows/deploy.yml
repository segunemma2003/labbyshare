name: Deploy to Production

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: labmyshare2020
          POSTGRES_DB: labmyshare_db
          POSTGRES_USER: labmyshare
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-django pytest-cov

      - name: Set up test environment
        run: |
          cp .env.example .env
          echo "DEBUG=True" >> .env
          echo "SECRET_KEY=test-secret-key-for-github-actions" >> .env
          echo "DB_NAME=labmyshare_db" >> .env
          echo "DB_USER=labmyshare" >> .env
          echo "DB_PASSWORD=labmyshare2020" >> .env
          echo "DB_HOST=localhost" >> .env
          echo "DB_PORT=5432" >> .env
          echo "REDIS_URL=redis://localhost:6379/1" >> .env
          echo "CELERY_BROKER_URL=redis://localhost:6379/0" >> .env
          echo "CELERY_RESULT_BACKEND=redis://localhost:6379/0" >> .env

      - name: Run migrations
        run: |
          python manage.py migrate --run-syncdb

      - name: Run tests
        run: |
          python manage.py test
          # Uncomment if you have pytest tests
          # pytest --cov=. --cov-report=xml

      - name: Upload coverage reports
        if: success()
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'

    permissions:
      contents: read
      packages: write

    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64

  deploy:
    needs: [test, build]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'

    environment:
      name: production
      url: http://backend.beautyspabyshea.co.uk:8080

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup SSH
        uses: webfactory/ssh-agent@v0.7.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts

      - name: Deploy to server
        run: |
          ssh ${{ secrets.SSH_USER }}@${{ secrets.SERVER_HOST }} << 'EOF'
            set -e
            
            # Navigate to application directory
            cd /var/www/labmyshare
            
            # Create backup of current deployment
            if [ -f docker-compose.yml ]; then
              cp docker-compose.yml docker-compose.yml.backup.$(date +%Y%m%d_%H%M%S)
            fi
            
            # Pull latest changes from repository
            git fetch origin
            git reset --hard origin/main
            
            # Create the HTTP-only nginx config that's missing
            mkdir -p docker/nginx
            cat > docker/nginx/http-only.conf << 'NGINXEOF
            
            # Create basic nginx.conf if missing
            cat > docker/nginx/nginx.conf << 'NGINXCONFEOF'
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;
    
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';
    
    access_log /var/log/nginx/access.log main;
    
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    server_tokens off;
    
    gzip on;
    gzip_vary on;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/javascript
        application/xml+rss
        application/json;
    
    include /etc/nginx/conf.d/*.conf;
}
NGINXCONFEOF'
upstream labmyshare {
    server web:8000;
}

# Rate limiting zones
limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
limit_req_zone $binary_remote_addr zone=auth:10m rate=5r/s;
limit_req_zone $binary_remote_addr zone=uploads:10m rate=2r/s;

server {
    listen 80;
    server_name backend.beautyspabyshea.co.uk localhost;
    
    # Security headers (HTTP version)
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;
    add_header Content-Security-Policy "default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self' data:; connect-src 'self';" always;
    
    # Request size limits
    client_max_body_size 10M;
    client_body_buffer_size 128k;
    
    # Timeouts
    proxy_connect_timeout 60s;
    proxy_send_timeout 60s;
    proxy_read_timeout 60s;
    
    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_comp_level 6;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/javascript
        application/xml+rss
        application/json
        image/svg+xml;
    
    # Main location - Django application
    location / {
        proxy_pass http://labmyshare;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Host $server_name;
        proxy_redirect off;
        
        # Enable proxy buffering
        proxy_buffering on;
        proxy_buffer_size 128k;
        proxy_buffers 4 256k;
        proxy_busy_buffers_size 256k;
    }
    
    # API endpoints with rate limiting
    location /api/v1/auth/ {
        limit_req zone=auth burst=10 nodelay;
        limit_req_status 429;
        
        proxy_pass http://labmyshare;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Host $server_name;
    }
    
    location /api/ {
        limit_req zone=api burst=20 nodelay;
        limit_req_status 429;
        
        proxy_pass http://labmyshare;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Host $server_name;
    }
    
    # File uploads with rate limiting
    location ~ ^/api/v1/(professionals/documents|admin/.*upload) {
        limit_req zone=uploads burst=5 nodelay;
        client_max_body_size 50M;
        
        proxy_pass http://labmyshare;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Host $server_name;
    }
    
    # Admin interface
    location /admin/ {
        proxy_pass http://labmyshare;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Host $server_name;
    }
    
    # Documentation
    location ~ ^/(swagger|redoc)/ {
        proxy_pass http://labmyshare;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Host $server_name;
    }
    
    # Static files
    location /static/ {
        alias /var/www/static/;
        expires 30d;
        add_header Cache-Control "public, no-transform";
        add_header Vary "Accept-Encoding";
        
        # Serve pre-compressed files if available
        location ~* \.(js|css)$ {
            gzip_static on;
        }
    }
    
    # Media files
    location /media/ {
        alias /var/www/media/;
        expires 7d;
        add_header Cache-Control "public, no-transform";
        
        # Security for uploaded files
        location ~* \.(php|phtml|pl|py|jsp|asp|sh|cgi)$ {
            deny all;
        }
    }
    
    # Health check - no rate limiting
    location /health/ {
        proxy_pass http://labmyshare;
        access_log off;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
    
    # Webhook endpoints
    location /api/v1/payments/webhook/ {
        proxy_pass http://labmyshare;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Host $server_name;
        
        # Stripe webhook specific headers
        proxy_set_header Stripe-Signature $http_stripe_signature;
    }
    
    # Robots.txt
    location = /robots.txt {
        add_header Content-Type text/plain;
        return 200 "User-agent: *\nDisallow: /admin/\nDisallow: /api/\n";
    }
    
    # Favicon
    location = /favicon.ico {
        log_not_found off;
        access_log off;
        return 204;
    }
    
    # Deny access to sensitive files
    location ~ /\. {
        deny all;
        access_log off;
        log_not_found off;
    }
    
    location ~ ~$ {
        deny all;
        access_log off;
        log_not_found off;
    }
}
NGINXEOF
            
            # Create the management command structure
            mkdir -p accounts/management/commands
            
            # Create __init__.py files
            touch accounts/management/__init__.py
            touch accounts/management/commands/__init__.py
            
            # Create the management command
            cat > accounts/management/commands/create_initial_data.py << 'PYTHONEOF'
import os
from django.core.management.base import BaseCommand
from django.contrib.auth import get_user_model
from django.db import transaction

User = get_user_model()

class Command(BaseCommand):
    help = 'Create initial data for the application'

    def handle(self, *args, **options):
        self.stdout.write(self.style.SUCCESS('Creating initial data...'))
        
        try:
            with transaction.atomic():
                # Create superuser if it doesn't exist
                self.create_superuser()
                
            self.stdout.write(
                self.style.SUCCESS('Successfully created initial data')
            )
        except Exception as e:
            self.stdout.write(
                self.style.ERROR(f'Error creating initial data: {str(e)}')
            )

    def create_superuser(self):
        """Create superuser if it doesn't exist"""
        admin_email = 'admin@labmyshare.com'
        admin_username = 'admin'
        admin_password = 'admin123'
        
        if not User.objects.filter(email=admin_email).exists():
            try:
                User.objects.create_superuser(
                    username=admin_username,
                    email=admin_email,
                    password=admin_password,
                    first_name='Admin',
                    last_name='User'
                )
                self.stdout.write(
                    self.style.SUCCESS(f'Superuser created: {admin_email} / {admin_password}')
                )
            except Exception as e:
                # If the User model doesn't have the expected fields, create a basic superuser
                self.stdout.write(
                    self.style.WARNING(f'Creating basic superuser due to: {str(e)}')
                )
                from django.contrib.auth.models import User as DjangoUser
                if not DjangoUser.objects.filter(username=admin_username).exists():
                    DjangoUser.objects.create_superuser(
                        username=admin_username,
                        email=admin_email,
                        password=admin_password
                    )
                    self.stdout.write(
                        self.style.SUCCESS(f'Basic superuser created: {admin_username} / {admin_password}')
                    )
        else:
            self.stdout.write(
                self.style.WARNING('Superuser already exists')
            )
PYTHONEOF
            
            # Create clean .env file - NO command injection possible here
            cat > .env << 'ENVEOF'
SECRET_KEY=${{ secrets.DJANGO_SECRET_KEY }}
DEBUG=False
ALLOWED_HOSTS=backend.beautyspabyshea.co.uk,localhost,127.0.0.1

# Database
DB_NAME=${{ secrets.DB_NAME }}
DB_USER=${{ secrets.DB_USER }}
DB_PASSWORD=${{ secrets.DB_PASSWORD }}
DB_HOST=${{ secrets.DB_HOST }}
DB_PORT=${{ secrets.DB_PORT }}

# Redis
REDIS_URL=${{ secrets.REDIS_URL }}
CELERY_BROKER_URL=${{ secrets.CELERY_BROKER_URL }}
CELERY_RESULT_BACKEND=${{ secrets.CELERY_RESULT_BACKEND }}

# Email
EMAIL_HOST=${{ secrets.EMAIL_HOST }}
EMAIL_PORT=${{ secrets.EMAIL_PORT }}
EMAIL_HOST_USER=${{ secrets.EMAIL_HOST_USER }}
EMAIL_HOST_PASSWORD=${{ secrets.EMAIL_HOST_PASSWORD }}
DEFAULT_FROM_EMAIL=${{ secrets.DEFAULT_FROM_EMAIL }}

# Firebase
FIREBASE_PROJECT_ID=${{ secrets.FIREBASE_PROJECT_ID }}
FIREBASE_PRIVATE_KEY_ID=${{ secrets.FIREBASE_PRIVATE_KEY_ID }}
FIREBASE_PRIVATE_KEY="${{ secrets.FIREBASE_PRIVATE_KEY }}"
FIREBASE_CLIENT_EMAIL=${{ secrets.FIREBASE_CLIENT_EMAIL }}
FIREBASE_CLIENT_ID=${{ secrets.FIREBASE_CLIENT_ID }}

# Stripe
STRIPE_PUBLISHABLE_KEY=${{ secrets.STRIPE_PUBLISHABLE_KEY }}
STRIPE_SECRET_KEY=${{ secrets.STRIPE_SECRET_KEY }}
STRIPE_WEBHOOK_SECRET=${{ secrets.STRIPE_WEBHOOK_SECRET }}

# Twilio
TWILIO_ACCOUNT_SID=${{ secrets.TWILIO_ACCOUNT_SID }}
TWILIO_AUTH_TOKEN=${{ secrets.TWILIO_AUTH_TOKEN }}
TWILIO_PHONE_NUMBER=${{ secrets.TWILIO_PHONE_NUMBER }}

# GitHub Repository for image
GITHUB_REPOSITORY=${{ github.repository }}
ENVEOF
            
            # Clean any previous problematic .env content
            sed -i '/echo.*docker/d' .env 2>/dev/null || true
            sed -i '/docker login/d' .env 2>/dev/null || true
            sed -i '/ghs_/d' .env 2>/dev/null || true
            
            # Log in to GitHub Container Registry
            echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u "${{ github.actor }}" --password-stdin
            
            # Copy the production docker-compose file
            cp docker-compose.production.yml docker-compose.yml
            
            # Create storage directories if they don't exist
            chmod +x docker/scripts/init-storage.sh
            ./docker/scripts/init-storage.sh
            
            # Complete cleanup and restart
            docker compose down -v --remove-orphans
            docker container prune -f
            docker image prune -f
            docker volume prune -f
            
            # Pull the latest image
            docker compose pull
            
            # Start new containers
            docker compose up -d --remove-orphans
            
            # Wait for services to be ready
            sleep 45
            
            # Run migrations
            docker compose exec -T web python manage.py migrate || echo "Migration failed, continuing..."
            
            # Collect static files
            docker compose exec -T web python manage.py collectstatic --noinput || echo "Static collection failed, continuing..."
            
            # Create initial data
            docker compose exec -T web python manage.py create_initial_data || echo "Initial data creation failed, continuing..."
            
            # Final restart to ensure everything is working
            docker compose restart nginx web
            
            # Clean up old images
            docker image prune -af --filter "until=24h"
            
            echo "Deployment completed successfully!"
          EOF

      - name: Health Check
        run: |
          sleep 60
          for i in {1..5}; do
            if curl -f http://backend.beautyspabyshea.co.uk:8080/health/; then
              echo "Health check passed"
              exit 0
            fi
            echo "Health check failed, attempt $i/5"
            sleep 10
          done
          echo "Health check failed after 5 attempts"
          exit 1

      - name: Notify deployment status
        if: always()
        run: |
          if [ "${{ job.status }}" == "success" ]; then
            echo "✅ Deployment successful to http://backend.beautyspabyshea.co.uk:8080"
          else
            echo "❌ Deployment failed"
          fi

  cleanup:
    needs: deploy
    runs-on: ubuntu-latest
    if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')

    steps:
      - name: Clean up old images
        run: |
          # Delete old package versions (keep last 5)
          gh api --method GET \
            -H "Accept: application/vnd.github+json" \
            -H "X-GitHub-Api-Version: 2022-11-28" \
            "/user/packages/container/$(echo ${{ github.repository }} | tr '[:upper:]' '[:lower:]')/versions" \
            --field per_page=100 | \
          jq -r '.[5:] | .[].id' | \
          head -20 | \
          xargs -I {} gh api --method DELETE \
            -H "Accept: application/vnd.github+json" \
            -H "X-GitHub-Api-Version: 2022-11-28" \
            "/user/packages/container/$(echo ${{ github.repository }} | tr '[:upper:]' '[:lower:]')/versions/{}" || echo "Cleanup completed"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
