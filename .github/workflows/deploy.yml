name: Deploy to Production

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: labmyshare2020
          POSTGRES_DB: labmyshare_db
          POSTGRES_USER: labmyshare
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-django pytest-cov

      - name: Set up test environment
        run: |
          cp .env.example .env
          echo "DEBUG=True" >> .env
          echo "SECRET_KEY=test-secret-key-for-github-actions" >> .env
          echo "DB_NAME=labmyshare_db" >> .env
          echo "DB_USER=labmyshare" >> .env
          echo "DB_PASSWORD=labmyshare2020" >> .env
          echo "DB_HOST=localhost" >> .env
          echo "DB_PORT=5432" >> .env
          echo "REDIS_URL=redis://localhost:6379/1" >> .env
          echo "CELERY_BROKER_URL=redis://localhost:6379/0" >> .env
          echo "CELERY_RESULT_BACKEND=redis://localhost:6379/0" >> .env

      - name: Run migrations
        run: |
          python manage.py migrate --run-syncdb

      - name: Run tests
        run: |
          python manage.py test

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'

    permissions:
      contents: read
      packages: write

    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64

  deploy:
    needs: [test, build]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'

    environment:
      name: production
      url: http://backend.beautyspabyshea.co.uk:8080

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup SSH
        uses: webfactory/ssh-agent@v0.7.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts

      - name: Create deployment files
        run: |
          # Create docker-compose.yml
          cat > docker-compose.fixed.yml << 'EOF'
          services:
            db:
              container_name: labmyshare_db
              image: postgres:15
              restart: unless-stopped
              environment:
                POSTGRES_DB: labmyshare_db
                POSTGRES_USER: labmyshare
                POSTGRES_PASSWORD: labmyshare2020
              volumes:
                - postgres_data:/var/lib/postgresql/data/
                - ./backups/postgres:/backups
              ports:
                - "127.0.0.1:5432:5432"
              healthcheck:
                test: ["CMD-SHELL", "pg_isready -U labmyshare -d labmyshare_db"]
                interval: 10s
                timeout: 5s
                retries: 5
                start_period: 30s
              networks:
                - labmyshare_network

            redis:
              container_name: labmyshare_redis
              image: redis:7-alpine
              restart: unless-stopped
              command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
              volumes:
                - redis_data:/data
              ports:
                - "127.0.0.1:6379:6379"
              healthcheck:
                test: ["CMD", "redis-cli", "ping"]
                interval: 10s
                timeout: 5s
                retries: 5
              networks:
                - labmyshare_network

            web:
              container_name: labmyshare_web
              image: ghcr.io/${{ github.repository }}:latest
              restart: unless-stopped
              command: >
                sh -c "
                  echo 'Waiting for database...' &&
                  until nc -z db 5432; do sleep 2; done &&
                  echo 'Database ready!' &&
                  python manage.py migrate --noinput &&
                  python manage.py collectstatic --noinput &&
                  gunicorn labmyshare.wsgi:application --bind 0.0.0.0:8000 --workers 4 --threads 2 --timeout 120
                "
              volumes:
                - static_volume:/app/staticfiles
                - media_volume:/app/media
                - logs_volume:/app/logs
              ports:
                - "127.0.0.1:8000:8000"
              env_file:
                - .env
              depends_on:
                db:
                  condition: service_healthy
                redis:
                  condition: service_healthy
              healthcheck:
                test: ["CMD", "curl", "-f", "http://localhost:8000/health/"]
                interval: 30s
                timeout: 10s
                retries: 5
                start_period: 90s
              networks:
                - labmyshare_network

            celery:
              container_name: labmyshare_celery
              image: ghcr.io/${{ github.repository }}:latest
              restart: unless-stopped
              command: >
                sh -c "
                  until nc -z db 5432 && nc -z redis 6379; do sleep 2; done &&
                  celery -A labmyshare worker --loglevel=info --concurrency=4
                "
              volumes:
                - media_volume:/app/media
                - logs_volume:/app/logs
              env_file:
                - .env
              depends_on:
                - db
                - redis
              healthcheck:
                test: ["CMD", "celery", "-A", "labmyshare", "inspect", "ping"]
                interval: 30s
                timeout: 10s
                retries: 3
              networks:
                - labmyshare_network

            celery-beat:
              container_name: labmyshare_celery_beat
              image: ghcr.io/${{ github.repository }}:latest
              restart: unless-stopped
              command: >
                sh -c "
                  until nc -z db 5432 && nc -z redis 6379; do sleep 2; done &&
                  celery -A labmyshare beat --loglevel=info --scheduler django_celery_beat.schedulers:DatabaseScheduler
                "
              volumes:
                - logs_volume:/app/logs
              env_file:
                - .env
              depends_on:
                - db
                - redis
              healthcheck:
                test: ["CMD", "pgrep", "-f", "celery.*beat"]
                interval: 30s
                timeout: 10s
                retries: 3
              networks:
                - labmyshare_network

            nginx:
              container_name: labmyshare_nginx
              image: nginx:alpine
              restart: unless-stopped
              ports:
                - "8080:80"
              volumes:
                - ./docker/nginx/http-only.conf:/etc/nginx/conf.d/default.conf:ro
                - ./docker/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
                - static_volume:/var/www/static:ro
                - media_volume:/var/www/media:ro
                - nginx_logs:/var/log/nginx
              depends_on:
                web:
                  condition: service_healthy
              networks:
                - labmyshare_network

            db-backup:
              container_name: labmyshare_db_backup
              image: postgres:15
              restart: unless-stopped
              environment:
                POSTGRES_DB: labmyshare_db
                POSTGRES_USER: labmyshare
                POSTGRES_PASSWORD: labmyshare2020
                PGPASSWORD: labmyshare2020
              volumes:
                - ./backups/postgres:/backups
              command: >
                sh -c "
                  until nc -z db 5432; do sleep 5; done &&
                  echo '0 2 * * * cd /backups && pg_dump -h db -U labmyshare -d labmyshare_db > backup_$$(date +%%Y%%m%%d_%%H%%M%%S).sql' | crontab - &&
                  crond -f
                "
              depends_on:
                db:
                  condition: service_healthy
              networks:
                - labmyshare_network

          volumes:
            postgres_data:
              driver: local
              driver_opts:
                type: none
                o: bind
                device: /var/www/labmyshare/storage/postgres
            redis_data:
              driver: local
              driver_opts:
                type: none
                o: bind
                device: /var/www/labmyshare/storage/redis
            static_volume:
              driver: local
              driver_opts:
                type: none
                o: bind
                device: /var/www/labmyshare/storage/static
            media_volume:
              driver: local
              driver_opts:
                type: none
                o: bind
                device: /var/www/labmyshare/storage/media
            logs_volume:
              driver: local
              driver_opts:
                type: none
                o: bind
                device: /var/www/labmyshare/storage/logs
            nginx_logs:
              driver: local
              driver_opts:
                type: none
                o: bind
                device: /var/www/labmyshare/storage/nginx-logs

          networks:
            labmyshare_network:
              driver: bridge
          EOF

          # Create nginx config
          cat > nginx.conf << 'EOF'
          user nginx;
          worker_processes auto;
          error_log /var/log/nginx/error.log warn;
          pid /var/run/nginx.pid;
          events {
            worker_connections 1024;
          }
          http {
            include /etc/nginx/mime.types;
            default_type application/octet-stream;
            sendfile on;
            keepalive_timeout 65;
            include /etc/nginx/conf.d/*.conf;
          }
          EOF

          # Create nginx site config
          cat > http-only.conf << 'EOF'
          upstream labmyshare {
            server web:8000;
          }
          server {
            listen 80;
            server_name backend.beautyspabyshea.co.uk localhost;
            
            location / {
              proxy_pass http://labmyshare;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Proto $scheme;
              proxy_connect_timeout 60s;
              proxy_send_timeout 60s;
              proxy_read_timeout 60s;
            }
            
            location /health/ {
              proxy_pass http://labmyshare;
              access_log off;
            }
            
            location /static/ {
              alias /var/www/static/;
              expires 30d;
            }
            
            location /media/ {
              alias /var/www/media/;
              expires 7d;
            }
          }
          EOF

          # Create management command
          cat > create_initial_data.py << 'EOF'
          from django.core.management.base import BaseCommand
          from django.contrib.auth import get_user_model
          
          User = get_user_model()
          
          class Command(BaseCommand):
              help = "Create initial data"
              
              def handle(self, *args, **options):
                  try:
                      admin_email = "admin@labmyshare.com"
                      if not User.objects.filter(email=admin_email).exists():
                          User.objects.create_superuser(
                              username="admin",
                              email=admin_email,
                              password="admin123"
                          )
                          print("âœ… Superuser created")
                      else:
                          print("â„¹ï¸  Superuser already exists")
                  except Exception as e:
                      print(f"âŒ Error: {e}")
          EOF

          # Create .env file
          cat > production.env << 'EOF'
          SECRET_KEY=ghfjgk.hl;iogulfkydjtxfgcvjbk.hllfkdtjxgchvjgkifuydcghvjgv
          DEBUG=False
          ALLOWED_HOSTS=backend.beautyspabyshea.co.uk,localhost,127.0.0.1
          DB_NAME=labmyshare_db
          DB_USER=labmyshare
          DB_PASSWORD=labmyshare2020
          DB_HOST=db
          DB_PORT=5432
          REDIS_URL=redis://redis:6379/1
          CELERY_BROKER_URL=redis://redis:6379/0
          CELERY_RESULT_BACKEND=redis://redis:6379/0
          EMAIL_HOST=smtp.resend.com
          EMAIL_PORT=587
          EMAIL_HOST_USER=resend
          EMAIL_HOST_PASSWORD=re_ZBwZZ2tj_AMtGvcpxoa1DofEXdV3BKM2f
          DEFAULT_FROM_EMAIL=noreply@beautyspabyshea.co.uk
          GITHUB_REPOSITORY=${{ github.repository }}
          EOF

      - name: Copy files to server
        run: |
          scp docker-compose.fixed.yml ${{ secrets.SSH_USER }}@${{ secrets.SERVER_HOST }}:/tmp/
          scp nginx.conf ${{ secrets.SSH_USER }}@${{ secrets.SERVER_HOST }}:/tmp/
          scp http-only.conf ${{ secrets.SSH_USER }}@${{ secrets.SERVER_HOST }}:/tmp/
          scp create_initial_data.py ${{ secrets.SSH_USER }}@${{ secrets.SERVER_HOST }}:/tmp/
          scp production.env ${{ secrets.SSH_USER }}@${{ secrets.SERVER_HOST }}:/tmp/

      - name: Deploy to Production Server
        run: |
          ssh ${{ secrets.SSH_USER }}@${{ secrets.SERVER_HOST }} 'bash -s' << 'DEPLOY_SCRIPT'
            set -e
            
            echo "ğŸš€ Starting LabMyShare deployment..."
            
            # Navigate to application directory
            cd /var/www/labmyshare
            
            # Backup current setup
            if [ -f docker-compose.yml ]; then
              cp docker-compose.yml docker-compose.yml.backup.$(date +%Y%m%d_%H%M%S)
              echo "âœ… Backup created"
            fi
            
            # Pull latest code
            echo "ğŸ“¥ Pulling latest code..."
            git fetch origin
            git reset --hard origin/main
            
            # Create directories
            echo "ğŸ“ Creating directories..."
            mkdir -p docker/nginx docker/scripts accounts/management/commands
            mkdir -p storage/{postgres,redis,static,media,logs,nginx-logs}
            mkdir -p backups/postgres
            
            # Set permissions
            chmod 755 storage/{redis,static,media,logs,nginx-logs}
            chmod 700 storage/postgres
            
            # Copy deployment files
            echo "ğŸ“‹ Setting up configuration files..."
            cp /tmp/docker-compose.fixed.yml docker-compose.yml
            cp /tmp/nginx.conf docker/nginx/nginx.conf
            cp /tmp/http-only.conf docker/nginx/http-only.conf
            cp /tmp/production.env .env
            
            # Setup management command
            touch accounts/management/__init__.py
            touch accounts/management/commands/__init__.py
            cp /tmp/create_initial_data.py accounts/management/commands/
            
            # Docker login
            echo "ğŸ³ Logging into Docker registry..."
            echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u "${{ github.actor }}" --password-stdin
            
            # Complete cleanup
            echo "ğŸ§¹ Cleaning up existing deployment..."
            docker compose down --volumes --remove-orphans || true
            
            # Stop any running containers
            CONTAINERS=$(docker ps -q)
            if [ ! -z "$CONTAINERS" ]; then
              docker stop $CONTAINERS
            fi
            docker container prune -f
            
            # Kill processes on ports
            echo "ğŸ”“ Freeing up ports..."
            sudo fuser -k 8000/tcp 2>/dev/null || true
            sudo fuser -k 8080/tcp 2>/dev/null || true
            sudo fuser -k 5432/tcp 2>/dev/null || true
            sudo fuser -k 6379/tcp 2>/dev/null || true
            
            sleep 10
            
            # Pull images
            echo "ğŸ“¥ Pulling Docker images..."
            docker compose pull
            
            # Start database and redis first
            echo "ğŸ—„ï¸ Starting core services..."
            docker compose up -d db redis
            
            # Wait for database
            echo "â³ Waiting for database..."
            timeout=60
            counter=0
            while [ $counter -lt $timeout ]; do
              if docker compose exec -T db pg_isready -U labmyshare -d labmyshare_db > /dev/null 2>&1; then
                echo "âœ… Database ready!"
                break
              fi
              counter=$((counter + 1))
              if [ $counter -eq $timeout ]; then
                echo "âŒ Database timeout"
                docker compose logs db
                exit 1
              fi
              echo "â³ Waiting... ($counter/$timeout)"
              sleep 1
            done
            
            # Ensure database exists
            echo "ğŸ”§ Verifying database..."
            docker compose exec -T db psql -U labmyshare -lqt | grep labmyshare_db || {
              echo "ğŸ“¦ Creating database..."
              docker compose exec -T db createdb -U labmyshare labmyshare_db
            }
            
            # Start all services
            echo "ğŸš€ Starting all services..."
            docker compose up -d
            
            # Wait for web service
            echo "â³ Waiting for web service to start..."
            sleep 60
            
            # Run Django setup
            echo "ğŸ“Š Running migrations..."
            docker compose exec -T web python manage.py migrate --noinput
            
            echo "ğŸ“¦ Collecting static files..."
            docker compose exec -T web python manage.py collectstatic --noinput
            
            echo "ğŸ‘¤ Creating initial data..."
            docker compose exec -T web python manage.py create_initial_data
            
            # Final restart
            echo "ğŸ”„ Restarting services..."
            docker compose restart web nginx
            sleep 20
            
            # Show status
            echo "ğŸ“Š Final status:"
            docker compose ps
            
            # Cleanup
            docker image prune -af --filter "until=24h"
            
            # Remove temp files
            rm /tmp/docker-compose.fixed.yml /tmp/nginx.conf /tmp/http-only.conf /tmp/create_initial_data.py /tmp/production.env
            
            echo "âœ… Deployment completed!"
          DEPLOY_SCRIPT

      - name: Health Check
        run: |
          echo "ğŸ” Running health checks..."
          sleep 30
          
          for i in {1..10}; do
            echo "ğŸ” Health check attempt $i/10..."
            if curl -f -m 10 http://backend.beautyspabyshea.co.uk:8080/health/; then
              echo "âœ… Health check passed!"
              exit 0
            fi
            echo "â³ Retrying in 15 seconds..."
            sleep 15
          done
          
          echo "âŒ Health check failed"
          ssh ${{ secrets.SSH_USER }}@${{ secrets.SERVER_HOST }} 'cd /var/www/labmyshare && docker compose ps && docker compose logs web --tail=20'
          exit 1

      - name: Deployment Summary
        if: always()
        run: |
          if [ "${{ job.status }}" == "success" ]; then
            echo "âœ… Deployment successful!"
            echo "ğŸŒ Application: http://backend.beautyspabyshea.co.uk:8080"
            echo "ğŸ” Health: http://backend.beautyspabyshea.co.uk:8080/health/"
            echo "ğŸ“š API docs: http://backend.beautyspabyshea.co.uk:8080/swagger/"
          else
            echo "âŒ Deployment failed"
          fi

  cleanup:
    needs: deploy
    runs-on: ubuntu-latest
    if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')

    steps:
      - name: Clean up old container images
        run: |
          gh api --method GET \
            -H "Accept: application/vnd.github+json" \
            -H "X-GitHub-Api-Version: 2022-11-28" \
            "/user/packages/container/$(echo ${{ github.repository }} | tr '[:upper:]' '[:lower:]')/versions" \
            --field per_page=100 | \
          jq -r '.[5:] | .[].id' | \
          head -20 | \
          xargs -I {} gh api --method DELETE \
            -H "Accept: application/vnd.github+json" \
            -H "X-GitHub-Api-Version: 2022-11-28" \
            "/user/packages/container/$(echo ${{ github.repository }} | tr '[:upper:]' '[:lower:]')/versions/{}" || echo "Cleanup completed"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
